{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73d13500",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Model for Wuthering Waves Google Play Reviews\n",
    "\n",
    "**Author:** Michael Teguh Carlo Simbolon  \n",
    "**Cohort ID:** MS155D5Y0583  \n",
    "**Date:** March 10, 2025\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook performs sentiment analysis on Google Play Store reviews for the game \"Wuthering Waves\". \n",
    "We explore and compare multiple approaches:\n",
    "\n",
    "1. Traditional ML with TF-IDF + SVM\n",
    "2. Deep Learning with Word Embeddings + LSTM\n",
    "3. Traditional ML with TF-IDF + Random Forest\n",
    "\n",
    "The goal is to classify reviews into three sentiment categories (positive, neutral, negative) with over 92% accuracy.\n",
    "Sentiment labeling is performed using a lexicon-based approach to ensure it reflects the text content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e0d85d",
   "metadata": {},
   "source": [
    "## 1. Setup and Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31530e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Deep learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119e159",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Sentiment Labeling\n",
    "\n",
    "We load the dataset and apply sentiment labeling using a lexicon-based approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9ecb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Load dataset\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading dataset...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m df = \u001b[43mpd\u001b[49m.read_csv(\u001b[33m'\u001b[39m\u001b[33mdataset.csv\u001b[39m\u001b[33m'\u001b[39m)  \u001b[38;5;66;03m# Adjust file name as needed\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset loaded with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# Apply sentiment labeling\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Define lexicon for sentiment analysis (Indonesian)\n",
    "positive_words = set(['bagus', 'seru', 'keren', 'suka', 'mantap', 'hebat', 'menarik', 'puas', 'lancar', 'keren'])\n",
    "negative_words = set(['buruk', 'jelek', 'lambat', 'bug', 'error', 'sulit', 'mengecewakan', 'bosan', 'lag', 'crash'])\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Clean text for sentiment analysis.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def label_sentiment(text):\n",
    "    \"\"\"Label sentiment based on lexicon.\"\"\"\n",
    "    tokens = preprocess_text(text)\n",
    "    score = 0\n",
    "    for token in tokens:\n",
    "        if token in positive_words:\n",
    "            score += 1\n",
    "        elif token in negative_words:\n",
    "            score -= 1\n",
    "    if score > 0:\n",
    "        return 'positif'\n",
    "    elif score < 0:\n",
    "        return 'negatif'\n",
    "    else:\n",
    "        return 'netral'\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading dataset...\")\n",
    "df = pd.read_csv('dataset.csv')  # Adjust file name as needed\n",
    "print(f\"Dataset loaded with {len(df)} samples.\")\n",
    "\n",
    "# Apply sentiment labeling\n",
    "print(\"Labeling sentiment based on review text...\")\n",
    "df['Label Sentimen'] = df['Text'].apply(label_sentiment)\n",
    "print(\"Sentiment labeling completed.\")\n",
    "\n",
    "# Display the first few rows\n",
    "print(\"\\nSample data:\")\n",
    "display(df[['Text', 'Label Sentimen']].head())\n",
    "\n",
    "# Check for missing values\n",
    "print(\"\\nMissing values:\")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fb1247",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "We clean and prepare the text data for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce8b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text_model(text):\n",
    "    \"\"\"Clean text for modeling.\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing for modeling\n",
    "print(\"Preprocessing text data for modeling...\")\n",
    "df['Processed_Text'] = df['Text'].apply(preprocess_text_model)\n",
    "print(\"Text preprocessing completed.\")\n",
    "\n",
    "# Display sample of processed text\n",
    "print(\"\\nSample of processed text:\")\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df['Text'].iloc[i]}\")\n",
    "    print(f\"Processed: {df['Processed_Text'].iloc[i]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f10100a",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "Analyze the distribution of sentiment classes and text characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930f90d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class distribution\n",
    "print(\"Sentiment class distribution:\")\n",
    "sentiment_counts = df['Label Sentimen'].value_counts()\n",
    "display(sentiment_counts)\n",
    "\n",
    "# Plot the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(x='Label Sentimen', data=df, palette='viridis')\n",
    "plt.title('Distribution of Sentiment Classes')\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate text length statistics\n",
    "df['text_length'] = df['Processed_Text'].apply(len)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(data=df, x='text_length', hue='Label Sentimen', bins=50, kde=True)\n",
    "plt.title('Distribution of Text Lengths by Sentiment')\n",
    "plt.xlabel('Text Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.axvline(x=df['text_length'].mean(), color='r', linestyle='--', label=f'Mean: {df[\"text_length\"].mean():.1f}')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07342547",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering\n",
    "\n",
    "Prepare features for both traditional ML and deep learning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45032590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Features and Labels\n",
    "X = df['Processed_Text']\n",
    "y = df['Label Sentimen'].map({'positif': 2, 'netral': 1, 'negatif': 0})  # Encode labels\n",
    "\n",
    "# TF-IDF Vectorization for traditional ML\n",
    "print(\"Creating TF-IDF features...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = tfidf.fit_transform(X).toarray()\n",
    "print(f\"TF-IDF feature shape: {X_tfidf.shape}\")\n",
    "\n",
    "# Text sequence preparation for LSTM\n",
    "print(\"\\nCreating sequence features for deep learning...\")\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(X)\n",
    "X_seq = tokenizer.texts_to_sequences(X)\n",
    "X_pad = pad_sequences(X_seq, maxlen=100)\n",
    "print(f\"Sequence feature shape: {X_pad.shape}\")\n",
    "\n",
    "# Display vocabulary size\n",
    "word_index = tokenizer.word_index\n",
    "print(f\"Vocabulary size: {len(word_index)} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dc64c8",
   "metadata": {},
   "source": [
    "## 6. Experiment 1: TF-IDF + SVM (80/20 Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd8ecd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Experiment 1: TF-IDF + SVM (80/20 Split) ===\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Train SVM model\n",
    "print(\"\\nTraining SVM model...\")\n",
    "svm_model = SVC(kernel='linear', probability=True)\n",
    "svm_model.fit(X_train, y_train)\n",
    "print(\"SVM model training completed.\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating SVM model...\")\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "svm_acc = accuracy_score(y_test, y_pred_svm)\n",
    "print(f\"SVM Accuracy: {svm_acc:.4f}\")\n",
    "\n",
    "# Display detailed classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm, target_names=['negatif', 'netral', 'positif']))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test, y_pred_svm)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negatif', 'netral', 'positif'], \n",
    "            yticklabels=['negatif', 'netral', 'positif'])\n",
    "plt.title('SVM Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd10d419",
   "metadata": {},
   "source": [
    "## 7. Experiment 2: Word Embedding + LSTM (80/20 Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca8d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Experiment 2: Word Embedding + LSTM (80/20 Split) ===\")\n",
    "\n",
    "# Split data\n",
    "X_train_lstm, X_test_lstm, y_train_lstm, y_test_lstm = train_test_split(X_pad, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set: {X_train_lstm.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_lstm.shape[0]} samples\")\n",
    "\n",
    "# Build LSTM model\n",
    "print(\"\\nBuilding LSTM model...\")\n",
    "lstm_model = Sequential([\n",
    "    Embedding(input_dim=5000, output_dim=128, input_length=100),\n",
    "    SpatialDropout1D(0.2),\n",
    "    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "lstm_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"LSTM model architecture:\")\n",
    "lstm_model.summary()\n",
    "\n",
    "# Train LSTM\n",
    "print(\"\\nTraining LSTM model...\")\n",
    "history = lstm_model.fit(\n",
    "    X_train_lstm, y_train_lstm,\n",
    "    epochs=10,\n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    verbose=1\n",
    ")\n",
    "print(\"LSTM model training completed.\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating LSTM model...\")\n",
    "lstm_loss, lstm_acc = lstm_model.evaluate(X_test_lstm, y_test_lstm, verbose=0)\n",
    "print(f\"LSTM Test Accuracy: {lstm_acc:.4f}\")\n",
    "\n",
    "y_pred_lstm = np.argmax(lstm_model.predict(X_test_lstm), axis=1)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_lstm, y_pred_lstm, target_names=['negatif', 'netral', 'positif']))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test_lstm, y_pred_lstm)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negatif', 'netral', 'positif'], \n",
    "            yticklabels=['negatif', 'netral', 'positif'])\n",
    "plt.title('LSTM Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c948a2d1",
   "metadata": {},
   "source": [
    "## 8. Experiment 3: TF-IDF + Random Forest (70/30 Split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed0c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Experiment 3: TF-IDF + Random Forest (70/30 Split) ===\")\n",
    "\n",
    "# Split data\n",
    "X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_tfidf, y, test_size=0.3, random_state=42)\n",
    "print(f\"Training set: {X_train_rf.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test_rf.shape[0]} samples\")\n",
    "\n",
    "# Train Random Forest\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_rf, y_train_rf)\n",
    "print(\"Random Forest model training completed.\")\n",
    "\n",
    "# Evaluate\n",
    "print(\"\\nEvaluating Random Forest model...\")\n",
    "y_pred_rf = rf_model.predict(X_test_rf)\n",
    "rf_acc = accuracy_score(y_test_rf, y_pred_rf)\n",
    "print(f\"Random Forest Accuracy: {rf_acc:.4f}\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test_rf, y_pred_rf, target_names=['negatif', 'netral', 'positif']))\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "cm = confusion_matrix(y_test_rf, y_pred_rf)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['negatif', 'netral', 'positif'], \n",
    "            yticklabels=['negatif', 'netral', 'positif'])\n",
    "plt.title('Random Forest Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f91db1",
   "metadata": {},
   "source": [
    "## 9. Model Deployment: Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b23252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model_type='lstm'):\n",
    "    \"\"\"Predict sentiment for new text input.\"\"\"\n",
    "    processed_text = preprocess_text_model(text)\n",
    "    label_map = {0: 'negatif', 1: 'netral', 2: 'positif'}\n",
    "    \n",
    "    if model_type == 'lstm':\n",
    "        seq = tokenizer.texts_to_sequences([processed_text])\n",
    "        padded = pad_sequences(seq, maxlen=100)\n",
    "        pred_probs = lstm_model.predict(padded)[0]\n",
    "        pred_label = np.argmax(pred_probs)\n",
    "        confidence = pred_probs[pred_label]\n",
    "    elif model_type == 'svm':\n",
    "        tfidf_text = tfidf.transform([processed_text]).toarray()\n",
    "        pred_label = svm_model.predict(tfidf_text)[0]\n",
    "        confidence = np.max(svm_model.predict_proba(tfidf_text)[0])\n",
    "    elif model_type == 'rf':\n",
    "        tfidf_text = tfidf.transform([processed_text]).toarray()\n",
    "        pred_label = rf_model.predict(tfidf_text)[0]\n",
    "        confidence = np.max(rf_model.predict_proba(tfidf_text)[0])\n",
    "    else:\n",
    "        raise ValueError(\"Model type must be 'lstm', 'svm', or 'rf'\")\n",
    "    \n",
    "    return label_map[pred_label], confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fd90c5",
   "metadata": {},
   "source": [
    "## 10. Testing Inference on Sample Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ea7e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_reviews = [\n",
    "    \"Game ini sangat seru dan grafisnya bagus!\",\n",
    "    \"Biasa saja, tidak terlalu menarik.\",\n",
    "    \"Banyak bug, sangat mengecewakan.\",\n",
    "    \"Gameplay cukup menarik tapi masih ada beberapa masalah teknis.\",\n",
    "    \"Karakter dan cerita sangat menarik, saya sangat menikmati permainan ini!\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"=== Inference Examples ===\")\n",
    "for review in sample_reviews:\n",
    "    lstm_pred, lstm_conf = predict_sentiment(review, 'lstm')\n",
    "    svm_pred, svm_conf = predict_sentiment(review, 'svm')\n",
    "    rf_pred, rf_conf = predict_sentiment(review, 'rf')\n",
    "    results.append({\n",
    "        'Review': review,\n",
    "        'LSTM Prediction': lstm_pred,\n",
    "        'LSTM Confidence': f\"{lstm_conf:.4f}\",\n",
    "        'SVM Prediction': svm_pred,\n",
    "        'SVM Confidence': f\"{svm_conf:.4f}\",\n",
    "        'RF Prediction': rf_pred,\n",
    "        'RF Confidence': f\"{rf_conf:.4f}\"\n",
    "    })\n",
    "\n",
    "display(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc06825a",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f9edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data = {\n",
    "    'Model': ['SVM + TF-IDF (80/20)', 'LSTM + Embedding (80/20)', 'RF + TF-IDF (70/30)'],\n",
    "    'Accuracy': [svm_acc, lstm_acc, rf_acc],\n",
    "    'Split Ratio': ['80/20', '80/20', '70/30'],\n",
    "    'Feature Type': ['TF-IDF', 'Word Embedding', 'TF-IDF']\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df['Accuracy'] = summary_df['Accuracy'].apply(lambda x: f\"{x:.4f}\")\n",
    "\n",
    "print(\"=== Summary of Results ===\")\n",
    "display(summary_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "accuracies = [svm_acc, lstm_acc, rf_acc]\n",
    "model_names = ['SVM + TF-IDF', 'LSTM + Embedding', 'RF + TF-IDF']\n",
    "bars = plt.bar(model_names, accuracies, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.axhline(y=0.92, color='r', linestyle='--', label='Target Accuracy (92%)')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.005, f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433d8b79",
   "metadata": {},
   "source": [
    "## 12. Future Improvements\n",
    "\n",
    "- Expand the sentiment lexicon with more Indonesian words.\n",
    "- Add negation handling (e.g., 'tidak bagus' as negative).\n",
    "- Use pre-trained embeddings like FastText for better feature representation.\n",
    "- Implement cross-validation for robust evaluation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
